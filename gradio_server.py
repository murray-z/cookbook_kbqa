
import gradio as gr
import json

from openai import OpenAI
from py2neo import Graph, Node, Relationship
from config import *

# è¿æ¥ Neo4j
graph = Graph(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))


system_prompt = """
ä½ æ˜¯ä¸€ä¸ªä¸­é¤çŸ¥è¯†å›¾è°±çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œåå­—å«â€˜å°é›·â€™ï¼Œæœ‰ä¸€ä¸ªneo4jçš„ä¸­é¤çŸ¥è¯†å›¾è°±æ•°æ®åº“ï¼Œå¯ä»¥ä¾›ä½ æŸ¥è¯¢ï¼Œä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

# Neo4jæ•°æ®åº“ç»“æ„
- Node: Category
    - èœå“å¤§ç±»ï¼Œä¾‹å¦‚ï¼šçº¢çƒ§è‚‰ç±»ã€æ„å¤§åˆ©é¢ç±»
    - å±æ€§
        - name: èœå“å¤§ç±»çš„åç§°
- Node: Dish
    - å…·ä½“çš„èœå“ï¼Œä¾‹å¦‚ï¼šå…ƒå®çº¢çƒ§è‚‰ã€è•ƒèŒ„ç«è…¿æ„é¢
    - å±æ€§
        - name: èœå“çš„åç§°
        - Flavor: èœå“çš„å£å‘³
        - Technique: èœå“çš„çƒ¹é¥ªæŠ€å·§
        - Time: èœå“çš„çƒ¹é¥ªæ—¶é—´
        - Difficulty: èœå“çš„çƒ¹é¥ªéš¾åº¦
        - Step: èœå“çš„çƒ¹é¥ªæ­¥éª¤
- Node: Ingredient
    - èœå“çš„é£Ÿæï¼Œä¾‹å¦‚ï¼šäº”èŠ±è‚‰ã€ç•ªèŒ„ã€ç«è…¿
    - å±æ€§
        - name: é£Ÿæçš„åç§°      
- Relationship: BELONGS_TO
    - èœå“å±äºæŸä¸ªèœå“å¤§ç±»
- Relationship: CONTAINS
    - èœå“åŒ…å«æŸä¸ªé£Ÿæ
"""

tools = [
    {
        "type": "function",
        "function": {
            "name": "get_neo4j_res",
            "description": "ä»èœè°±çŸ¥è¯†å›¾è°±æ•°æ®åº“è·å–ç›¸å…³æŸ¥è¯¢ç»“æœ",
            "parameters": {
                "type": "object",
                "properties": {
                    "cypher_query": {
                        "type": "string",
                        "description": "Cypher æŸ¥è¯¢è¯­å¥"
                    }
                },
                "required": ["cypher_query"]
            }
        }
    }
]


def get_neo4j_res(cypher_query):
    result = graph.run(cypher_query).data()
    return json.dumps(result, ensure_ascii=False, indent=2)


def llm(query, history):
    client = OpenAI(
        api_key=API_KEY,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )

    messages = [{"role": "system", "content": system_prompt}]
    if history:
        for user, bot in history[-3:]:
            messages.append({"role": "user", "content": user})
            messages.append({"role": "assistant", "content": bot})

    messages.append({"role": "user", "content": query})

    completion = client.chat.completions.create(
        model=MODEL_NAME,
        messages=messages,
        tools=tools,
        stream=True
    )

    response = ""
    tool_calls = []

    for chunk in completion:
        if chunk.choices[0].finish_reason == "stop" and not chunk.choices[0].delta.content:
            print(chunk)
            break

        delta = chunk.choices[0].delta
        if delta.tool_calls:  # æ£€æµ‹å·¥å…·è°ƒç”¨
            tool_calls.extend(delta.tool_calls)
        elif delta.content:  # æ­£å¸¸æ–‡æœ¬è¾“å‡º
            response += delta.content
            yield response  # é€æ­¥è¿”å›å†…å®¹

    print(tool_calls)
    # å¤„ç†å·¥å…·è°ƒç”¨
    if tool_calls:
        args = ""
        func_name = ""
        for tool_call in tool_calls:
            chunk_args = tool_call.function.arguments
            if chunk_args:
                args += tool_call.function.arguments
            if tool_call.function.name:
                func_name = tool_call.function.name

        print(args, func_name)
        if func_name == "get_neo4j_res":
            arguments = json.loads(args)

            # å°†ç³»ç»Ÿæç¤ºåŒ…è£…ä¸ºæŠ˜å é¡¹çš„ HTML
            tip_msg = (
                f"<details style='margin-bottom: 10px;'>"
                f"<summary style='font-size: 16px; color: #2c3e50;'>ğŸ”§ã€ç³»ç»Ÿæç¤ºã€‘è°ƒç”¨å·¥å…· {func_name}ï¼Œç‚¹å‡»å±•å¼€æŸ¥çœ‹è¯¦æƒ…</summary>\n\n"
                f"<strong>è°ƒç”¨å‚æ•°ï¼š</strong>\n"
                f"<pre style='background-color: #f7f7f7; border: 1px solid #ccc; padding: 10px;'>{json.dumps(arguments, ensure_ascii=False, indent=2)}</pre>\n"
                f"</details>"
            )

            response += tip_msg
            yield response  # è¿”å›æŠ˜å é¡¹çš„ç³»ç»Ÿæç¤º

            tool_response = get_neo4j_res(**arguments)
            print(f"tool_response: {tool_response}")

            tip_result = (
                f"<details style='margin-bottom: 10px;'>"
                f"<summary style='font-size: 16px; color: #2c3e50;'>ğŸ”§ã€ç³»ç»Ÿæç¤ºã€‘å·¥å…· {func_name} è¿”å›ç»“æœï¼Œç‚¹å‡»å±•å¼€æŸ¥çœ‹è¯¦æƒ…</summary>\n\n"
                f"<strong>è¿”å›ç»“æœï¼š</strong>\n"
                f"<pre style='background-color: #f7f7f7; border: 1px solid #ccc; padding: 10px;'>{tool_response}</pre>\n"
                f"</details>"
            )

            response += tip_result
            yield response  # è¿”å›æŠ˜å é¡¹çš„å·¥å…·è¿”å›ç»“æœ

            tool_calls = [{"id": "tool_call_1", "function": {"name": func_name, "arguments": args}}]

            messages.append({"role": "assistant", "content": None, "tool_calls": tool_calls})
            messages.append({"role": "tool", "name": "get_neo4j_res", "content": tool_response})

            # ç»§ç»­å¯¹è¯
            completion = client.chat.completions.create(
                model=MODEL_NAME,
                messages=messages,
                stream=True
            )

            for chunk in completion:
                if chunk.choices[0].finish_reason == "stop" and not chunk.choices[0].delta.content:
                    break
                delta = chunk.choices[0].delta
                if delta.content:
                    response += delta.content
                    yield response  # é€æ­¥è¿”å›æœ€ç»ˆå†…å®¹


def chat_with_ai(user_input, history):
    response = ""
    for chunk in llm(user_input, history):
        response += chunk
        yield chunk  # åªè¿”å›å¢é‡


# ä½¿ç”¨ Gradio Blocks API åˆ›å»ºç•Œé¢
with gr.Blocks() as demo:
    gr.Markdown("""
# ğŸœ ä¸­é¤çŸ¥è¯†å°åŠ©æ‰‹ â€”â€” å°é›· ğŸ‘¨â€ğŸ³  

ä½ å¥½ï¼æˆ‘æ˜¯ **å°é›·**ï¼Œä½ çš„ **ä¸­é¤æ™ºèƒ½åŠ©æ‰‹**ã€‚æˆ‘è¿æ¥ç€ä¸€ä¸ª **å¼ºå¤§çš„ä¸­é¤çŸ¥è¯†å›¾è°±æ•°æ®åº“**ï¼Œå¯ä»¥å¸®åŠ©ä½ æ¢ç´¢ **ä¸­åç¾é£Ÿçš„å¥¥ç§˜**ï¼  

### ğŸ“š **æˆ‘èƒ½åšä»€ä¹ˆï¼Ÿ**  
âœ… **æ¨èèœå“**ï¼šä¸çŸ¥é“åƒä»€ä¹ˆï¼Ÿæˆ‘å¯ä»¥å¸®ä½ æŒ‘é€‰é€‚åˆä½ çš„ä¸­é¤ç¾é£Ÿï¼  
âœ… **æŸ¥è¯¢çƒ¹é¥ªæ–¹æ³•**ï¼šæƒ³è‡ªå·±åŠ¨æ‰‹åšä¸€é“ç¾å‘³çš„èœï¼Ÿæˆ‘å¯ä»¥æä¾›è¯¦ç»†çš„ **æ­¥éª¤ã€æŠ€å·§ã€æ—¶é—´å’Œéš¾åº¦**ï¼  
âœ… **é£Ÿææ­é…å»ºè®®**ï¼šä¸ç¡®å®šæŸç§é£Ÿææ€ä¹ˆæ­é…ï¼Ÿæˆ‘å¯ä»¥å‘Šè¯‰ä½  **å“ªäº›é£Ÿæèƒ½ç¢°æ’å‡ºç»ä½³å£æ„Ÿ**ï¼  

### ğŸ› **æˆ‘çš„çŸ¥è¯†åº“**  
ğŸ“Œ **èœå“åˆ†ç±»**ï¼šçº¢çƒ§è‚‰ç±»ã€æ„å¤§åˆ©é¢ç±»â€¦â€¦å„ç§é£å‘³åº”æœ‰å°½æœ‰ï¼  
ğŸ“Œ **èœå“ä¿¡æ¯**ï¼šå£å‘³ã€çƒ¹é¥ªæŠ€å·§ã€åˆ¶ä½œæ—¶é—´ã€éš¾åº¦ã€è¯¦ç»†æ­¥éª¤ï¼Œä¸€ç›®äº†ç„¶ï¼  
ğŸ“Œ **é£ŸæçŸ¥è¯†**ï¼šä»äº”èŠ±è‚‰åˆ°ç•ªèŒ„ï¼Œä»ç«è…¿åˆ°ç§˜åˆ¶é¦™æ–™ï¼Œé£Ÿææ­é…ä¸å†éš¾ï¼  

ğŸ“¢ **æ— è®ºæ˜¯æ¢ç´¢ç»å…¸ä¸­é¤ï¼Œè¿˜æ˜¯å¯»æ‰¾çƒ¹é¥ªçµæ„Ÿï¼Œå¿«æ¥å’Œå°é›·èŠèŠå§ï¼** ğŸ‰  
""")
    chatbot = gr.Chatbot()
    user_input = gr.Textbox(label="è¯·è¾“å…¥ä½ çš„é—®é¢˜")
    history = gr.State([])


    def respond(message, chat_history):
        chat_history.append((message, ""))  # å…ˆå ä½
        for chunk in chat_with_ai(message, chat_history):
            chat_history[-1] = (message, chunk)  # åªæ›´æ–°æœ€æ–°çš„ chunk
            yield chat_history, chat_history, ""


    user_input.submit(respond, [user_input, history], [chatbot, history, user_input])

demo.launch(share=True)
